

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gradient Descent &mdash; Software Engineering for Machine Learning  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Support Vector Machines" href="chapter_svm.html" />
    <link rel="prev" title="Supervised Machine Learning" href="chapter_supervised.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Software Engineering for Machine Learning
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../frontmatter/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_python.html">Introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_vc.html">Version Control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_cr.html">Code Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_test.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_ws.html">Web Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_ci.html">Continuous Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_graphics.html">Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_ml.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_kmeans.html">K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_hierarchical.html">Hierarchical Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_pca.html">Principal Component Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_practical.html">Practical Considerations of Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_floating_point.html">Floating Point and Finite Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_supervised.html">Supervised Machine Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gradient Descent</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#linear-approximation-with-least-square-error">Linear Approximation with Least Square Error</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#analytics-method-for-least-square-error">Analytics Method for Least Square Error</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradient">Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Gradient Descent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#numerical-method-for-least-square-error">Numerical Method for Least Square Error</a></li>
<li class="toctree-l3"><a class="reference internal" href="#numerical-method-for-least-square-error-of-quadratic-function">Numerical Method for Least Square Error of Quadratic Function</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-descent-for-neural-networks">Gradient Descent for Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#three-layer-neural-networks">Three-Layer Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gradient-descent-in-neural-networks-backpropagation">Gradient Descent in Neural Networks (Backpropagation)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#chain-rule">Chain Rule</a></li>
<li class="toctree-l4"><a class="reference internal" href="#error-and-weights-between-output-and-hidden-layers">Error and Weights between Output and Hidden Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#error-and-weights-between-hidden-and-input-layers">Error and Weights between Hidden and Input Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neural-networks-as-logic-gates">Neural Networks as Logic Gates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_svm.html">Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_neural_networks.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../misc/chapter_building_the_book.html">Building the Book using Sphinx, GitHub Pages, and Travis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../misc/chapter_sphinx_demo.html">Sphinx Demonstration</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Software Engineering for Machine Learning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Gradient Descent</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/supervised/chapter_gradient.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="gradient-descent">
<h1>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h1>
<p>Learning Objectives</p>
<ul class="simple">
<li><p>Understand partial derivative and gradient</p></li>
<li><p>Understand how gradient descent may be used in optimization problems</p></li>
<li><p>Apply gradient descent in machine learning</p></li>
</ul>
<div class="section" id="linear-approximation-with-least-square-error">
<h2>Linear Approximation with Least Square Error<a class="headerlink" href="#linear-approximation-with-least-square-error" title="Permalink to this headline">¶</a></h2>
<p>As mentioned in the previous section, supervised machinear learning
can be formulated as a minimization problem: minimizing the
error. This chapter starts with a problem that is probably farmiliar
to many people already: <em>linear approximation with least square
error</em>.</p>
<p>Consider a list of <span class="math notranslate nohighlight">\(n\)</span> points: <span class="math notranslate nohighlight">\((x_1, \tilde{y_1})\)</span>,
<span class="math notranslate nohighlight">\((x_2, \tilde{y_2})\)</span>, …, <span class="math notranslate nohighlight">\((x_n, \tilde{y_n})\)</span>.  Please
notice the convention: <span class="math notranslate nohighlight">\(y = a x + b\)</span> is the underlying equation
and <span class="math notranslate nohighlight">\(y\)</span> is the “correct” value. It is generally not possible
getting the correct value of <span class="math notranslate nohighlight">\(y\)</span> due to noise and limitation of
meausrement instruments. Instead, we can get only the observed
<span class="math notranslate nohighlight">\(y\)</span> with noise.  To distinguish these two, we use
<span class="math notranslate nohighlight">\(\tilde{y}\)</span> to express the observed value. It may be different
from the true value of <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>The problem is to find the values of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> for a
line <span class="math notranslate nohighlight">\(y = a x + b\)</span> such that</p>
<p><span class="math notranslate nohighlight">\(e(a, b)= \underset{i=1}{\overset{n}{\sum}} (y_i - (a x_i +   b))^2\)</span></p>
<p>is as small as possible. This is the cumulative error. Let’s call it
<span class="math notranslate nohighlight">\(e(a, b)\)</span> because it has two variables <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.
Here we will solve this problem in two ways: analytically and
numerically.</p>
<div class="section" id="analytics-method-for-least-square-error">
<h3>Analytics Method for Least Square Error<a class="headerlink" href="#analytics-method-for-least-square-error" title="Permalink to this headline">¶</a></h3>
<p>In Calculus, you have learned the concept of derivative. Suppose
<span class="math notranslate nohighlight">\(f(x)\)</span> is a function of a single variable <span class="math notranslate nohighlight">\(x\)</span>. The
derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> is defined as</p>
<p><span class="math notranslate nohighlight">\(f'(x) = \frac{d}{dx} f(x) = \underset{h \rightarrow 0}{\text{lim}} \frac{f(x + h) - f(x)}{h}\)</span></p>
<p>The derivative calculates the ratio of change in <span class="math notranslate nohighlight">\(f(x)\)</span> and the
change in <span class="math notranslate nohighlight">\(x\)</span>. A geometric interpretation is the slope of
<span class="math notranslate nohighlight">\(f(x)\)</span> at a specific point of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Extend that concept to a multivariable function. Suppose <span class="math notranslate nohighlight">\(f(x,
y)\)</span> is a function of two variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The
partial derivative of <span class="math notranslate nohighlight">\(f(x,y)\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> at
point <span class="math notranslate nohighlight">\((x_0, y_0)\)</span> is defined as</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}| _{(x_0, y_0)} = \frac{d}{dx} f(x, y_0) | _{x = x_0} =\underset{h \rightarrow 0}{\text{lim}} \frac{f(x_0 + h, y_0) - f(x_0, y_0)}{h}\)</span></p>
<p>The derivative calculates the ratio of change in <span class="math notranslate nohighlight">\(f(x, y)\)</span> and the
change in <span class="math notranslate nohighlight">\(x\)</span> <em>while keeping the value of</em> <span class="math notranslate nohighlight">\(y\)</span> <em>unchanged</em>.</p>
<p>Similarly, the partial derivative of <span class="math notranslate nohighlight">\(f(x,y)\)</span> with respect to
<span class="math notranslate nohighlight">\(y\)</span> at point <span class="math notranslate nohighlight">\((x_0, y_0)\)</span> is defined as</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y}| _{(x_0, y_0)} = \frac{d}{dy} f(x_0, y) | _{y = y_0} =\underset{h \rightarrow 0}{\text{lim}} \frac{f(x_0, y_0 + h) - f(x_0, y_0)}{h}\)</span></p>
<p>To minimize the error function, we take the partial derivatives of
<span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> respectively:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial a}  = 2 (a  x_1 + b - y_1)  x_1 + 2 (a  x_2 + b - y_2)  x_2 + ... + 2 (a  x_n + b - y_n)  x_n\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial a}  = 2 a (x_1^2 + x_2^2 + ... + x_n^2) + 2 b (x_1 + x_2 + ... + x_n) - 2 (x_1 y_1 + x_2 y_2 + ... + x_n y_n)\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial a}  = 2 (a \underset{i=1}{\overset{n}{\sum}} x_i^2 + b \underset{i=1}{\overset{n}{\sum}} x_i - \underset{i=1}{\overset{n}{\sum}} x_i y_i)\)</span></p>
<p>For <span class="math notranslate nohighlight">\(b\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial b} = 2 (a  x_1 + b - y_1) + 2 (a  x_2 + b - y_2) + ... + 2 (a \times x_n + b - y_n)\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial b} = 2 a (x_1 + x_2 + ... + x_n) + 2 n b  - 2 (y_1 + y_2 + ... + y_n)\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial b} = 2 (a \underset{i=1}{\overset{n}{\sum}} x_i + b n - \underset{i=1}{\overset{n}{\sum}} y_i)\)</span></p>
<p>To find the minimum, set both to zero and obtain two linear equations of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p><span class="math notranslate nohighlight">\(a \underset{i=1}{\overset{n}{\sum}} x_i^2 + b \underset{i=1}{\overset{n}{\sum}} x_i = \underset{i=1}{\overset{n}{\sum}} x_i y_i\)</span></p>
<p><span class="math notranslate nohighlight">\(a \underset{i=1}{\overset{n}{\sum}} x_i + b n = \underset{i=1}{\overset{n}{\sum}} y_i\)</span></p>
<p>The values of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> can be expressed by</p>
<p><span class="math notranslate nohighlight">\(a =\frac{n \underset{i=1}{\overset{n}{\sum}} x_i y_i - \underset{i=1}{\overset{n}{\sum}} x_i \underset{i=1}{\overset{n}{\sum}} y_i}{n \underset{i=1}{\overset{n}{\sum}} x_i^2 - \underset{i=1}{\overset{n}{\sum}} x_i \underset{i=1}{\overset{n}{\sum}} x_i}\)</span></p>
<p><span class="math notranslate nohighlight">\(b =\frac{\underset{i=1}{\overset{n}{\sum}} y_i \underset{i=1}{\overset{n}{\sum}} x_i^2 - \underset{i=1}{\overset{n}{\sum}} x_i y_i \underset{i=1}{\overset{n}{\sum}} x_i}{n \underset{i=1}{\overset{n}{\sum}} x_i^2 - \underset{i=1}{\overset{n}{\sum}} x_i \underset{i=1}{\overset{n}{\sum}} x_i}\)</span></p>
<p>Let’s consider an example:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 48%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>x</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>8.095698376</p></td>
<td><p>23.51683637</p></td>
</tr>
<tr class="row-odd"><td><p>6.358470914</p></td>
<td><p>9.792790054</p></td>
</tr>
<tr class="row-even"><td><p>-9.053869996</p></td>
<td><p>-39.96474572</p></td>
</tr>
<tr class="row-odd"><td><p>-8.718226575</p></td>
<td><p>-30.35310844</p></td>
</tr>
<tr class="row-even"><td><p>8.92002599</p></td>
<td><p>26.16662601</p></td>
</tr>
<tr class="row-odd"><td><p>-9.304226583</p></td>
<td><p>-37.4223126</p></td>
</tr>
<tr class="row-even"><td><p>-4.413344816</p></td>
<td><p>-18.03547019</p></td>
</tr>
<tr class="row-odd"><td><p>9.24473846</p></td>
<td><p>24.39367474</p></td>
</tr>
<tr class="row-even"><td><p>2.717746556</p></td>
<td><p>-4.589498946</p></td>
</tr>
<tr class="row-odd"><td><p>5.87537092</p></td>
<td><p>15.7037148</p></td>
</tr>
<tr class="row-even"><td><p>-2.962047549</p></td>
<td><p>-7.508042385</p></td>
</tr>
<tr class="row-odd"><td><p>-1.793005634</p></td>
<td><p>-11.81506333</p></td>
</tr>
<tr class="row-even"><td><p>-2.341379964</p></td>
<td><p>-14.96321124</p></td>
</tr>
<tr class="row-odd"><td><p>4.742625547</p></td>
<td><p>2.282082477</p></td>
</tr>
<tr class="row-even"><td><p>-2.007598497</p></td>
<td><p>-5.068305913</p></td>
</tr>
<tr class="row-odd"><td><p>9.333353675</p></td>
<td><p>28.44940642</p></td>
</tr>
<tr class="row-even"><td><p>2.570708237</p></td>
<td><p>3.086379154</p></td>
</tr>
<tr class="row-odd"><td><p>-4.846225403</p></td>
<td><p>-25.6409577</p></td>
</tr>
<tr class="row-even"><td><p>2.571789981</p></td>
<td><p>7.795844519</p></td>
</tr>
<tr class="row-odd"><td><p>-9.044770879</p></td>
<td><p>-26.25061389</p></td>
</tr>
<tr class="row-even"><td><p>5.09385439</p></td>
<td><p>8.166196092</p></td>
</tr>
<tr class="row-odd"><td><p>-5.665252693</p></td>
<td><p>-21.99241714</p></td>
</tr>
<tr class="row-even"><td><p>1.193065754</p></td>
<td><p>0.698347441</p></td>
</tr>
<tr class="row-odd"><td><p>-8.739601542</p></td>
<td><p>-31.96384225</p></td>
</tr>
<tr class="row-even"><td><p>-5.850434065</p></td>
<td><p>-17.51926158</p></td>
</tr>
<tr class="row-odd"><td><p>4.556308579</p></td>
<td><p>9.854628779</p></td>
</tr>
<tr class="row-even"><td><p>-0.509866694</p></td>
<td><p>-10.85684654</p></td>
</tr>
<tr class="row-odd"><td><p>-0.24261641</p></td>
<td><p>-8.33876201</p></td>
</tr>
<tr class="row-even"><td><p>7.930407455</p></td>
<td><p>19.56805947</p></td>
</tr>
<tr class="row-odd"><td><p>6.201498841</p></td>
<td><p>5.836888055</p></td>
</tr>
<tr class="row-even"><td><p>-3.524341584</p></td>
<td><p>-19.45328039</p></td>
</tr>
<tr class="row-odd"><td><p>6.034477356</p></td>
<td><p>19.15245129</p></td>
</tr>
</tbody>
</table>
<p>The pairs are plotted below:</p>
<div class="figure align-default">
<img alt="../_images/xy1.png" src="../_images/xy1.png" />
</div>
<p>The value of <span class="math notranslate nohighlight">\(y\)</span> is calculated by</p>
<p><span class="math notranslate nohighlight">\(y = 3 x - 5 + \epsilon\)</span></p>
<p>here <span class="math notranslate nohighlight">\(\epsilon\)</span> is the error (or noise) and it is set to a randeom number between -8 and 8.</p>
<p>The figure shows the line without noise:</p>
<div class="figure align-default">
<img alt="../_images/xy2.png" src="../_images/xy2.png" />
</div>
<p>Using the equations, <span class="math notranslate nohighlight">\(a = 3.11806\)</span> and <span class="math notranslate nohighlight">\(b = -5.18776\)</span>.</p>
<p>Next, we explain how to solve the problem using <em>gradient descent</em>.</p>
</div>
</div>
<div class="section" id="gradient">
<h2>Gradient<a class="headerlink" href="#gradient" title="Permalink to this headline">¶</a></h2>
<p>The <em>gradient</em> of a two-variable function <span class="math notranslate nohighlight">\(f(x, y)\)</span> is at point <span class="math notranslate nohighlight">\((x_0, y_0)\)</span> is defined as</p>
<p><span class="math notranslate nohighlight">\(\nabla f|_{(x_0, y_0)} = \frac{\partial f}{\partial x} {\bf i} + \frac{\partial f}{\partial y} {\bf j}\)</span></p>
<p>Here, <span class="math notranslate nohighlight">\({\bf i}\)</span> and <span class="math notranslate nohighlight">\({\bf j}\)</span> are the unit vector in the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> directions.</p>
<p>Suppose <span class="math notranslate nohighlight">\({\bf v} = \alpha {\bf i} + \beta {\bf j}\)</span> is a unit
vector. Then, the amount of <span class="math notranslate nohighlight">\(f\)</span>’s changes in the direction of
<span class="math notranslate nohighlight">\({\bf v}\)</span> is the inner product of <span class="math notranslate nohighlight">\(\nabla f\)</span> and
<span class="math notranslate nohighlight">\({\bf v}\)</span>.  The greatest change occurs at the direction when
<span class="math notranslate nohighlight">\({\bf v}\)</span> is the unit vector of <span class="math notranslate nohighlight">\(\nabla f\)</span>.  Since
<span class="math notranslate nohighlight">\({\bf v}\)</span> is a unit vector, if its direction is different from
<span class="math notranslate nohighlight">\(\nabla f\)</span>, the inner product must be smaller.</p>
<p>One way to understand graident is to think about speed bumps on roads.</p>
<div class="figure align-default">
<img alt="../_images/roadbump.png" src="../_images/roadbump.png" />
</div>
<p>The bump is modeled as a half cylinder. For simplicity, we assume that
the bump is infinitely long in the <span class="math notranslate nohighlight">\(x\)</span> direction. A point on the
surface of the bump can be expressed as</p>
<p><span class="math notranslate nohighlight">\(p = x {\bf i} + \alpha \cos(\theta) {\bf j} + \beta \sin(\theta) {\bf k}\)</span></p>
<p>The gradient is</p>
<p><span class="math notranslate nohighlight">\(\nabla p = \frac{\partial p}{\partial x} {\bf i} + \frac{\partial p}{\partial y} {\bf j} + \frac{\partial p}{\partial z} {\bf k} = {\bf i} + \alpha (- \sin(\theta)) {\bf j} + \beta \cos(\theta) {\bf k}.\)</span></p>
<p>This is the <em>tangent</em> of the point on the surface.</p>
<p>Next, consider another vector (such as the tire of your unicycle) goes
through this bump.  We consider a unicycle because it is simpler: we
need to consider only one wheel. What is the rate of changes along
this surface? If you ride the unicycle along this bump without getting
onto the bump, then the vector of your movement can be expressed by</p>
<p><span class="math notranslate nohighlight">\(v = x {\bf i}\)</span></p>
<p>How is this affected by the slope of the bump? The calculation is the <em>inner product</em> of the two vectors:</p>
<p><span class="math notranslate nohighlight">\(\nabla p \cdot v = x {\bf i}\)</span>.</p>
<p>Notice that this inner product contains no <span class="math notranslate nohighlight">\(\theta\)</span>. What does
this mean? It means that your movement is not affected by
<span class="math notranslate nohighlight">\(\theta\)</span>. This is obvious because you are not riding onto the
bump.</p>
<p>Next, consider that you ride straight to the bump. The vector will be</p>
<p><span class="math notranslate nohighlight">\(v = - y {\bf j}\)</span></p>
<p>The slope of the bump affects your actual movement, again by the inner product:</p>
<p><span class="math notranslate nohighlight">\(\nabla p \cdot v = y \alpha \sin(\theta) {\bf j}\)</span>.</p>
<p>How can we interpret this? The moment when your tire hits the bump,
<span class="math notranslate nohighlight">\(\theta\)</span> is zero so your tire’s movement along the <span class="math notranslate nohighlight">\({\bf
j}\)</span> direction is zero. This is understandable because you cannot
penetrate into the bump.  When the tire is at the top of the bump,
<span class="math notranslate nohighlight">\(\theta\)</span> is <span class="math notranslate nohighlight">\(\frac{\pi}{2}\)</span> and the tire has the highest
speed.</p>
<p>Based on this understanding, it is easier to answer the following
question: “Which direction along the surface gives the greatest
changes?”  Because the actual change is the inner product of the
direction and the gradient, the greatest change occurs along the
direction of the gradient.</p>
</div>
<div class="section" id="id1">
<h2>Gradient Descent<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><em>Gradient Descent</em> is a method for solving <em>minimization</em> problems.
The gradient of a function at a point is the rate of changes at that
point.  Let’s consider a simple example: use gradient descent to find
a line that has the smallest sum of square error. This is the same
problem described above. The earlier solution uses formulae to find
<span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. Here, we will not use the formulae.</p>
<p>The gradient of a function is the direction of changes.</p>
<p><span class="math notranslate nohighlight">\(\nabla e = \frac{\partial e}{\partial a} {\bf i} + \frac{\partial e}{\partial b} {\bf j}\)</span></p>
<p>Suppose <span class="math notranslate nohighlight">\(\Delta w = \alpha {\bf i} + \beta {\bf j}\)</span> is a vector.</p>
<p>By definition, if <span class="math notranslate nohighlight">\(\Delta w\)</span> is small enough, then the change in
<span class="math notranslate nohighlight">\(\nabla e\)</span> alogn the direction of <span class="math notranslate nohighlight">\(\Delta w\)</span> can be
calculated as</p>
<p><span class="math notranslate nohighlight">\(\Delta e = \nabla e \cdot \Delta w\)</span>.</p>
<p>The goal is to reduce the error. Thus, <span class="math notranslate nohighlight">\(\Delta w\)</span> should be chosen to ensure that <span class="math notranslate nohighlight">\(\Delta e\)</span> is negative.
If we make</p>
<p><span class="math notranslate nohighlight">\(\Delta w = - \eta \nabla e\)</span>,</p>
<p>then</p>
<p><span class="math notranslate nohighlight">\(\Delta e = \nabla e \cdot (- \eta \nabla e) = - \eta (\nabla e)^2\)</span>.</p>
<p>This ensures that the error <span class="math notranslate nohighlight">\(e\)</span> becomes smaller.  The value
<span class="math notranslate nohighlight">\(\eta\)</span> is called the <em>learning rate</em>.  Its value is usually
between 0.1 and 0.5.  If <span class="math notranslate nohighlight">\(\eta\)</span> is too small, <span class="math notranslate nohighlight">\(\Delta e\)</span>
changes very slowly, i.e., learning is slow.  If <span class="math notranslate nohighlight">\(\eta\)</span> is too
large, <span class="math notranslate nohighlight">\(\Delta e = \nabla e \cdot \Delta w\)</span> is not necessarily
true.</p>
<div class="section" id="numerical-method-for-least-square-error">
<h3>Numerical Method for Least Square Error<a class="headerlink" href="#numerical-method-for-least-square-error" title="Permalink to this headline">¶</a></h3>
<p>It is possible to find an analytical solution for <span class="math notranslate nohighlight">\(a\)</span> and
<span class="math notranslate nohighlight">\(b\)</span> because the function <span class="math notranslate nohighlight">\(e\)</span> is pretty simple.  For many
machine learning problems, the functions are highly complex and in
many cases the functions are not even known in advance. For these
problems, reducing the errors can be done numerically using data.
This section is further divided into two different scenarios.</p>
<ul class="simple">
<li><p>The first assumes that we know the function <span class="math notranslate nohighlight">\(e\)</span> but we do not have formulaes for <span class="math notranslate nohighlight">\(a\)</span> or <span class="math notranslate nohighlight">\(b\)</span>.</p></li>
<li><p>The second assumes that we do not know the function <span class="math notranslate nohighlight">\(e\)</span> and certainly do not know the formulaes for <span class="math notranslate nohighlight">\(a\)</span> or <span class="math notranslate nohighlight">\(b\)</span>.  This is the common scenario.</p></li>
</ul>
<p>For the first case,</p>
<p><span class="math notranslate nohighlight">\(\nabla e = \frac{\partial e}{\partial a} {\bf i} + \frac{\partial e}{\partial b} {\bf j}\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial a} = 2 (a \underset{i=1}{\overset{n}{\sum}} x_i^2 + b \underset{i=1}{\overset{n}{\sum}} x_i - \underset{i=1}{\overset{n}{\sum}} x_i y_i)\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial b} = 2 (a \underset{i=1}{\overset{n}{\sum}} x_i + b n - \underset{i=1}{\overset{n}{\sum}} y_i)\)</span></p>
<p>This is <code class="docutils literal notranslate"><span class="pre">gradient1</span></code> below.</p>
<p>For the second case, the gradient can be estimated using the definition of partial derivative. This is shown in
<code class="docutils literal notranslate"><span class="pre">gradient2</span></code> below.</p>
<p>After finding the gradient using either method, the values of
<span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> change by <span class="math notranslate nohighlight">\(- \eta \nabla e\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/python3</span>
<span class="c1"># gradientdescent.py</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">argparse</span>

<span class="k">def</span> <span class="nf">readfile</span><span class="p">(</span><span class="n">file</span><span class="p">):</span>
    <span class="c1"># print(file)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">fhd</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">)</span> <span class="c1"># file handler</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">data</span>
    <span class="k">for</span> <span class="n">oneline</span> <span class="ow">in</span> <span class="n">fhd</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">oneline</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">readfiles</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">readfile</span><span class="p">(</span><span class="n">f1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">readfile</span><span class="p">(</span><span class="n">f2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">sums</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;ERROR&quot;</span><span class="p">)</span>
    <span class="n">sumx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sumx2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sumy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sumxy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">sumx</span> <span class="o">=</span> <span class="n">sumx</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">sumy</span> <span class="o">=</span> <span class="n">sumy</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">sumx2</span> <span class="o">=</span> <span class="n">sumx2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">sumxy</span> <span class="o">=</span> <span class="n">sumxy</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">sumx</span><span class="p">,</span> <span class="n">sumy</span><span class="p">,</span> <span class="n">sumx2</span><span class="p">,</span> <span class="n">sumxy</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gradient1</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">sumx</span><span class="p">,</span> <span class="n">sumy</span><span class="p">,</span> <span class="n">sumx2</span><span class="p">,</span> <span class="n">sumxy</span><span class="p">):</span>
    <span class="n">pa</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">sumx2</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">sumx</span> <span class="o">-</span> <span class="n">sumxy</span><span class="p">)</span>
    <span class="n">pb</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">sumx</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">n</span> <span class="o">-</span> <span class="n">sumy</span><span class="p">)</span>
    <span class="c1"># convert to unit vector</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pa</span> <span class="o">*</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">pb</span> <span class="o">*</span> <span class="n">pb</span><span class="p">)</span>
    <span class="n">ua</span> <span class="o">=</span> <span class="n">pa</span> <span class="o">/</span> <span class="n">length</span>
    <span class="n">ub</span> <span class="o">=</span> <span class="n">pb</span> <span class="o">/</span> <span class="n">length</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">ua</span><span class="p">,</span> <span class="n">ub</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gradient2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># use the definition</span>
    <span class="c1"># the partial derivative of function f respect to variable a is</span>
    <span class="c1"># (f(a + h) - f(a)) / h for small h</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">error0</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">errora</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">errorb</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">diff0</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">diffa</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="p">((</span><span class="n">a</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">diffb</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">+</span> <span class="n">h</span><span class="p">))</span>
        <span class="n">error0</span> <span class="o">=</span> <span class="n">error0</span> <span class="o">+</span> <span class="n">diff0</span> <span class="o">*</span> <span class="n">diff0</span>
        <span class="n">errora</span> <span class="o">=</span> <span class="n">errora</span> <span class="o">+</span> <span class="n">diffa</span> <span class="o">*</span> <span class="n">diffa</span>
        <span class="n">errorb</span> <span class="o">=</span> <span class="n">errorb</span> <span class="o">+</span> <span class="n">diffb</span> <span class="o">*</span> <span class="n">diffb</span>
    <span class="n">pa</span> <span class="o">=</span> <span class="p">(</span><span class="n">errora</span> <span class="o">-</span> <span class="n">error0</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">pb</span> <span class="o">=</span> <span class="p">(</span><span class="n">errorb</span> <span class="o">-</span> <span class="n">error0</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="c1"># convert to unit vector</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pa</span> <span class="o">*</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">pb</span> <span class="o">*</span> <span class="n">pb</span><span class="p">)</span>
    <span class="n">ua</span> <span class="o">=</span> <span class="n">pa</span> <span class="o">/</span> <span class="n">length</span>
    <span class="n">ub</span> <span class="o">=</span> <span class="n">pb</span> <span class="o">/</span> <span class="n">length</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">ua</span><span class="p">,</span> <span class="n">ub</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">findab</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">readfiles</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">file1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">file2</span><span class="p">)</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    method 1: know the formula for the gradient</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">sumx</span><span class="p">,</span> <span class="n">sumy</span><span class="p">,</span> <span class="n">sumx2</span><span class="p">,</span> <span class="n">sumxy</span><span class="p">]</span> <span class="o">=</span> <span class="n">sums</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># initial values for a and b</span>
    <span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">count</span> <span class="o">=</span>  <span class="mi">1000</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="p">[</span><span class="n">pa</span><span class="p">,</span> <span class="n">pb</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient1</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">sumx</span><span class="p">,</span> <span class="n">sumy</span><span class="p">,</span> <span class="n">sumx2</span><span class="p">,</span> <span class="n">sumxy</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pa</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pb</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    method 2: do not know the formula</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="mi">15</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">21</span>
    <span class="n">count</span> <span class="o">=</span>  <span class="mi">1000</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="p">[</span><span class="n">pa</span><span class="p">,</span> <span class="n">pb</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pa</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pb</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">checkArgs</span><span class="p">(</span><span class="n">args</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
  <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;parse arguments&#39;</span><span class="p">)</span>
  <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f1&#39;</span><span class="p">,</span> <span class="s1">&#39;--file1&#39;</span><span class="p">,</span> 
                      <span class="n">help</span> <span class="o">=</span> <span class="s1">&#39;name of the first data file&#39;</span><span class="p">,</span>
                      <span class="n">default</span> <span class="o">=</span> <span class="s1">&#39;xval&#39;</span><span class="p">)</span>
  <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f2&#39;</span><span class="p">,</span> <span class="s1">&#39;--file2&#39;</span><span class="p">,</span> 
                      <span class="n">help</span> <span class="o">=</span> <span class="s1">&#39;name of the second data file&#39;</span><span class="p">,</span>
                      <span class="n">default</span> <span class="o">=</span> <span class="s1">&#39;yval&#39;</span><span class="p">)</span>
  <span class="n">pargs</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">pargs</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
  <span class="n">args</span> <span class="o">=</span> <span class="n">checkArgs</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
  <span class="n">findab</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

</pre></div>
</div>
<p>The two methods get similar results: The first method gets 3.153 and
-5.187 for <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> respectively.  The second method
gets 3.027 and -5.192.</p>
</div>
<div class="section" id="numerical-method-for-least-square-error-of-quadratic-function">
<h3>Numerical Method for Least Square Error of Quadratic Function<a class="headerlink" href="#numerical-method-for-least-square-error-of-quadratic-function" title="Permalink to this headline">¶</a></h3>
<p>The examples above uses a linear function <span class="math notranslate nohighlight">\(y = a x + b\)</span>. Now,
let’s consider a more complex example: a quadratic function:
<span class="math notranslate nohighlight">\(y = a x^2  + b x + c\)</span>. The error function is defined
by three terms: <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, and <span class="math notranslate nohighlight">\(c\)</span>.</p>
<p><span class="math notranslate nohighlight">\(e(a, b, c)= \underset{i=1}{\overset{n}{\sum}} (y_i - (a x_i ^ 2 +  b x_i + c))^2\)</span></p>
<p>The value of <span class="math notranslate nohighlight">\(y\)</span> is calculated by</p>
<p><span class="math notranslate nohighlight">\(y = 3 x^2 - 5 x + 4 + \epsilon\)</span></p>
<p>here <span class="math notranslate nohighlight">\(\epsilon\)</span> is the error (or noise) and it is set to a randeom number between -8 and 8.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 48%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>x</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>-3.370103805</p></td>
<td><p>60.79185325</p></td>
</tr>
<tr class="row-odd"><td><p>0.417414305</p></td>
<td><p>-5.323619187</p></td>
</tr>
<tr class="row-even"><td><p>-6.159102555</p></td>
<td><p>154.3634865</p></td>
</tr>
<tr class="row-odd"><td><p>-4.407403766</p></td>
<td><p>82.5948879</p></td>
</tr>
<tr class="row-even"><td><p>9.217171131</p></td>
<td><p>211.5745285</p></td>
</tr>
<tr class="row-odd"><td><p>-7.174813026</p></td>
<td><p>188.278774</p></td>
</tr>
<tr class="row-even"><td><p>6.361795959</p></td>
<td><p>88.5112383</p></td>
</tr>
<tr class="row-odd"><td><p>1.79223335</p></td>
<td><p>11.99774387</p></td>
</tr>
<tr class="row-even"><td><p>-1.926842791</p></td>
<td><p>26.55005374</p></td>
</tr>
<tr class="row-odd"><td><p>3.11961916</p></td>
<td><p>12.0001045</p></td>
</tr>
<tr class="row-even"><td><p>3.407105822</p></td>
<td><p>25.23458989</p></td>
</tr>
<tr class="row-odd"><td><p>3.516276644</p></td>
<td><p>27.86258459</p></td>
</tr>
<tr class="row-even"><td><p>-6.446048868</p></td>
<td><p>153.4091251</p></td>
</tr>
<tr class="row-odd"><td><p>-1.611122918</p></td>
<td><p>16.29112594</p></td>
</tr>
<tr class="row-even"><td><p>-9.423197392</p></td>
<td><p>320.9271831</p></td>
</tr>
<tr class="row-odd"><td><p>-0.163191843</p></td>
<td><p>1.413342642</p></td>
</tr>
<tr class="row-even"><td><p>-7.984372477</p></td>
<td><p>237.3069972</p></td>
</tr>
<tr class="row-odd"><td><p>-1.083286108</p></td>
<td><p>20.77621844</p></td>
</tr>
<tr class="row-even"><td><p>-1.721374317</p></td>
<td><p>28.35361787</p></td>
</tr>
<tr class="row-odd"><td><p>9.307151676</p></td>
<td><p>211.0397578</p></td>
</tr>
<tr class="row-even"><td><p>2.847589125</p></td>
<td><p>15.49036578</p></td>
</tr>
<tr class="row-odd"><td><p>-9.704318719</p></td>
<td><p>333.9723539</p></td>
</tr>
<tr class="row-even"><td><p>6.652947501</p></td>
<td><p>106.6501811</p></td>
</tr>
<tr class="row-odd"><td><p>1.286261333</p></td>
<td><p>-2.972412887</p></td>
</tr>
<tr class="row-even"><td><p>6.730248976</p></td>
<td><p>104.7116952</p></td>
</tr>
<tr class="row-odd"><td><p>4.198544935</p></td>
<td><p>29.78821666</p></td>
</tr>
<tr class="row-even"><td><p>8.730545018</p></td>
<td><p>183.571542</p></td>
</tr>
<tr class="row-odd"><td><p>-5.944582098</p></td>
<td><p>142.8709084</p></td>
</tr>
<tr class="row-even"><td><p>3.596351215</p></td>
<td><p>19.40151</p></td>
</tr>
<tr class="row-odd"><td><p>-5.606758376</p></td>
<td><p>132.9978749</p></td>
</tr>
<tr class="row-even"><td><p>-7.680210557</p></td>
<td><p>215.0928846</p></td>
</tr>
<tr class="row-odd"><td><p>1.852911853</p></td>
<td><p>2.423800574</p></td>
</tr>
</tbody>
</table>
<p>The pairs are plotted below:</p>
<div class="figure align-default">
<img alt="../_images/quadratic.png" src="../_images/quadratic.png" />
</div>
<p>The program for calculating <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, and <span class="math notranslate nohighlight">\(c\)</span>
needs only slight changes from the previous program:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/python3</span>
<span class="c1"># gradientdescent2.py</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">argparse</span>

<span class="k">def</span> <span class="nf">readfile</span><span class="p">(</span><span class="n">file</span><span class="p">):</span>
    <span class="c1"># print(file)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">fhd</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">)</span> <span class="c1"># file handler</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">data</span>
    <span class="k">for</span> <span class="n">oneline</span> <span class="ow">in</span> <span class="n">fhd</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">oneline</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">readfiles</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">readfile</span><span class="p">(</span><span class="n">f1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">readfile</span><span class="p">(</span><span class="n">f2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">quadraticvalue</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">c</span>
    <span class="k">return</span> <span class="n">val</span>

<span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># use the definition</span>
    <span class="c1"># the partial derivative of function f respect to variable a is</span>
    <span class="c1"># (f(a + h) - f(a)) / h for small h</span>
    <span class="c1"># h = random.random() * 0.5 # make it small</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">error0</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">errora</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">errorb</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">errorc</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">diff0</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="n">quadraticvalue</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
        <span class="n">diffa</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="n">quadraticvalue</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">h</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
        <span class="n">diffb</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="n">quadraticvalue</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
        <span class="n">diffc</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="n">quadraticvalue</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
        <span class="n">error0</span> <span class="o">=</span> <span class="n">error0</span> <span class="o">+</span> <span class="n">diff0</span> <span class="o">*</span> <span class="n">diff0</span>
        <span class="n">errora</span> <span class="o">=</span> <span class="n">errora</span> <span class="o">+</span> <span class="n">diffa</span> <span class="o">*</span> <span class="n">diffa</span>
        <span class="n">errorb</span> <span class="o">=</span> <span class="n">errorb</span> <span class="o">+</span> <span class="n">diffb</span> <span class="o">*</span> <span class="n">diffb</span>
        <span class="n">errorc</span> <span class="o">=</span> <span class="n">errorc</span> <span class="o">+</span> <span class="n">diffc</span> <span class="o">*</span> <span class="n">diffc</span>
    <span class="n">pa</span> <span class="o">=</span> <span class="p">(</span><span class="n">errora</span> <span class="o">-</span> <span class="n">error0</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">pb</span> <span class="o">=</span> <span class="p">(</span><span class="n">errorb</span> <span class="o">-</span> <span class="n">error0</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="p">(</span><span class="n">errorc</span> <span class="o">-</span> <span class="n">error0</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="c1"># convert to unit vector</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pa</span> <span class="o">*</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">pb</span> <span class="o">*</span> <span class="n">pb</span> <span class="o">+</span> <span class="n">pc</span> <span class="o">*</span> <span class="n">pc</span><span class="p">)</span>
    <span class="n">ua</span> <span class="o">=</span> <span class="n">pa</span> <span class="o">/</span> <span class="n">length</span>
    <span class="n">ub</span> <span class="o">=</span> <span class="n">pb</span> <span class="o">/</span> <span class="n">length</span>
    <span class="n">uc</span> <span class="o">=</span> <span class="n">pc</span> <span class="o">/</span> <span class="n">length</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">ua</span><span class="p">,</span> <span class="n">ub</span><span class="p">,</span> <span class="n">uc</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">findabc</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">readfiles</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">file1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">file2</span><span class="p">)</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    do not know the formula</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span>  <span class="mi">10000</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="p">[</span><span class="n">pa</span><span class="p">,</span> <span class="n">pb</span><span class="p">,</span> <span class="n">pc</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pa</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pb</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">c</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pc</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">checkArgs</span><span class="p">(</span><span class="n">args</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
  <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;parse arguments&#39;</span><span class="p">)</span>
  <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f1&#39;</span><span class="p">,</span> <span class="s1">&#39;--file1&#39;</span><span class="p">,</span> 
                      <span class="n">help</span> <span class="o">=</span> <span class="s1">&#39;name of the first data file&#39;</span><span class="p">,</span>
                      <span class="n">default</span> <span class="o">=</span> <span class="s1">&#39;xval2&#39;</span><span class="p">)</span>
  <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f2&#39;</span><span class="p">,</span> <span class="s1">&#39;--file2&#39;</span><span class="p">,</span> 
                      <span class="n">help</span> <span class="o">=</span> <span class="s1">&#39;name of the second data file&#39;</span><span class="p">,</span>
                      <span class="n">default</span> <span class="o">=</span> <span class="s1">&#39;yval2&#39;</span><span class="p">)</span>
  <span class="n">pargs</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">pargs</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
  <span class="n">args</span> <span class="o">=</span> <span class="n">checkArgs</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
  <span class="n">findabc</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

</pre></div>
</div>
<p>Running this program ten times gets the following results:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 31%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>a</p></th>
<th class="head"><p>b</p></th>
<th class="head"><p>c</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>3.011255896382324</p></td>
<td><p>-5.237109053403006</p></td>
<td><p>4.587766159787751</p></td>
</tr>
<tr class="row-odd"><td><p>2.9118463569721067</p></td>
<td><p>-5.235739889402345</p></td>
<td><p>4.558316574435434</p></td>
</tr>
<tr class="row-even"><td><p>2.913418168901663</p></td>
<td><p>-5.236030395939285</p></td>
<td><p>4.611313910795599</p></td>
</tr>
<tr class="row-odd"><td><p>3.01028934524673 9</p></td>
<td><p>-5.237423283136554</p></td>
<td><p>4.6514776776523155</p></td>
</tr>
<tr class="row-even"><td><p>3.0116767376997156</p></td>
<td><p>-5.2370171800113</p></td>
<td><p>4.568807336762989</p></td>
</tr>
<tr class="row-odd"><td><p>3.0116145023680554</p></td>
<td><p>-5.237035419533481</p></td>
<td><p>4.572520167800626</p></td>
</tr>
<tr class="row-even"><td><p>2.911538900320818</p></td>
<td><p>-5.2359055455148615</p></td>
<td><p>4.591420135857997</p></td>
</tr>
<tr class="row-odd"><td><p>2.908676740232057</p></td>
<td><p>-5.2363349865534214</p></td>
<td><p>4.682182248171528</p></td>
</tr>
<tr class="row-even"><td><p>2.911471635130624</p></td>
<td><p>-5.235840824838787</p></td>
<td><p>4.5789355660578455</p></td>
</tr>
<tr class="row-odd"><td><p>2.911385934228692</p></td>
<td><p>-5.235874673892769</p></td>
<td><p>4.585754511975301</p></td>
</tr>
</tbody>
</table>
<p>The calculated values are pretty consistent.</p>
</div>
</div>
<div class="section" id="gradient-descent-for-neural-networks">
<h2>Gradient Descent for Neural Networks<a class="headerlink" href="#gradient-descent-for-neural-networks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="three-layer-neural-networks">
<h3>Three-Layer Neural Networks<a class="headerlink" href="#three-layer-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>Neural networks are the foundation of the recent impressive progress
in machine learning. The problems described so far have pretty clear
mathematical foundations (for example, linear approximation or
quadratic approximation). More complex problems can be difficult to
express mathematically. Let’s consider an example in our everyday
life: How do you describe a “chair”? “Simple”, you may say. You define
“A chair has four legs with a flat surface on which a person can sit,
like this figure”:</p>
<div class="figure align-default">
<img alt="../_images/chair1.jpg" src="../_images/chair1.jpg" />
</div>
<p>If that is a chair, how about this one?  “Well, that is also a
chair.”, you may say.</p>
<div class="figure align-default">
<img alt="../_images/chair2.jpg" src="../_images/chair2.jpg" />
</div>
<p>You change your answer to “A chair has two or four legs with a flat
surface on which a person can sit.”  There are still problems: How about
the following two? Are they chairs?</p>
<div class="figure align-default">
<img alt="../_images/chair3.png" src="../_images/chair3.png" />
</div>
<div class="figure align-default">
<img alt="../_images/chair4.jpg" src="../_images/chair4.jpg" />
</div>
<p>As you can see, it is not so easy to define a chair.</p>
<p>The same problem occurs in many other cases. How do you define a car?
You may say, “A car is a vehicle with four wheels and can transport
people.”  Some cars have only three wheels. Some cars have six
wheels. Some cars do not transport people.</p>
<p>Instead of giving a definition, another approach is to give many
examples and a complex enough function so that the function can learn
the common characteristics of these examples.  Neural networks are
complex <em>non-linear</em> functions and they have the capabilities of
learning from many examples.  More details about neural networks will
be covered in later chapters. This chapter gives only a few simple
examples showing how gradient descent can be used to learn.</p>
<p>Neural networks use one particular architecture for computing.  This
architecture is inspired by animal brains where billions of neurons
are connected.  This book does not intend to explain how neural
networks resemble brains, nor the biological experiments discovering the
mechanisms of brain functions.  Instead, this section solve relatively
simple problems (creating logic gates) using this particular
architecture.</p>
<p>The following figure is an example of a neural network with three
layers: an input layer, an output layer, and a hidden layer. A layer
is hidden simply because it is neither the input layer nor the output
layer.</p>
<div class="figure align-default" id="id2">
<img alt="../_images/layers.png" src="../_images/layers.png" />
<p class="caption"><span class="caption-number">Figure 65 </span><span class="caption-text">Neural network of three layers</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>In this example, the input layer has two neurons (neurons 0-1), the
hidden layer has four neurons (neurons 0-3), and the output layer has
two neurons (neurons 0-1).  Prior studies show that a neural network
with only two layers (input and output) without any hidden layer has
limited capability. Thus, almost all neural networks have hidden
layers.  Some complex networks can have dozens or even hundreds of
hidden layers.  It has been shown that <em>deep</em> neural networks can be
effective learning complex tasks. Here, deep means neural networks
have many hidden layers.  Information moves from the layers on the
left toward the layers on the right.  There is no feedback of
information.  Nor do two neurons of the same layer exchange
information.  Different layers may have different numbers of neurons.</p>
<div class="figure align-default" id="id3">
<img alt="../_images/forward.png" src="../_images/forward.png" />
<p class="caption"><span class="caption-number">Figure 66 </span><span class="caption-text">Connects neurons of different layers.  These are the only connections between enurons.  Please notice that the numbers of neurons are 2, 4, and 2 in the three layers.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>The neurons in two adjacent layers are connected by weights, are
expressed as a three-dimensional array:</p>
<p><span class="math notranslate nohighlight">\(weights[l][s][d]\)</span></p>
<p>connects the neuron at layer <span class="math notranslate nohighlight">\(l\)</span> to layer <span class="math notranslate nohighlight">\(l + 1\)</span>;
<span class="math notranslate nohighlight">\(s\)</span> is the source and <span class="math notranslate nohighlight">\(d\)</span> is the destination. It is also
written as <span class="math notranslate nohighlight">\(w_{l, s, d}\)</span>.  Please notice that the indexes start
from zero.</p>
<div class="figure align-default" id="id4">
<img alt="../_images/weights.png" src="../_images/weights.png" />
<p class="caption"><span class="caption-number">Figure 67 </span><span class="caption-text">Definition of the indexes for weights</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>In the example, the left weight <span class="math notranslate nohighlight">\(weights[0][1][2]\)</span> has the first
index 0 because it connects the input layer (layer 0) with the hidden
layer (layer 1). From the top, this is the second neuron of the input
layer; thus, the second index is 1.  The destination is the third
neuron in the hidden layer and the index is 2.  The right weight is
<span class="math notranslate nohighlight">\(weights[1][3][0]\)</span>. The first index is 1 because it is from the
hidden layer (layer 1). The second index is 3 because the source is
the fourth neuron from the top. The destination is the first neuron in
the output layer; thus, the third index is 0.</p>
<p>Each neuron performs relatively simple calculation, as shown below.</p>
<div class="figure align-default" id="id5">
<img alt="../_images/neuron.png" src="../_images/neuron.png" />
<p class="caption"><span class="caption-number">Figure 68 </span><span class="caption-text">Computation performed by one neuron</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>A neuron in the input layer has only one input (only <span class="math notranslate nohighlight">\(x_i\)</span>) and
the weight is always 1. A neuron in the hidden or the output layer has
multiple inputs.  Each neuron performs three stages of
computation. First, the products of the input and the weight are
added.</p>
<p><span class="math notranslate nohighlight">\(\underset{k = 0}{\overset{n-1}{\Sigma}} x_k w_k\)</span>.</p>
<p>Next, a constant called <em>bias</em> is added</p>
<p><span class="math notranslate nohighlight">\(b + \underset{k = 0}{\overset{n-1}{\Sigma}} x_k w_k\)</span>.</p>
<p>The value is then passed to an <em>activation function</em> <span class="math notranslate nohighlight">\(f\)</span>.  This
must be a non-linear function because linear functions are too limited
in their ability to handle complex situations.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The world is not linear. Imagine that you really love chocolate. If
you eat one piece of chocolate, you feel pretty good. If you eat
two pieces of chocolate, you feel even better. Does this mean you
feel better and better after you eat more and more chocolate?
No. After you eat a lof of chocolate, you may feel that’s enough
and want to stop. The same situation applies to everything. Imagine
you get paid $20 an hour. If you work for two hours, you get
$40. Can you make more and more money if you work longer and
longer? No. First, each day has only 24 hours. You can make at most
$480 per day if you do not sleep.  If you do not sleep, you will
fall ill soon and have to stop working.</p>
</div>
<p>Several activation
functions are popular. This section uses the <em>sigmoid</em> function,
expressed as <span class="math notranslate nohighlight">\(\sigma(x)\)</span>, is one of the most widely used:</p>
<p><span class="math notranslate nohighlight">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span>.</p>
<p>Because <span class="math notranslate nohighlight">\(e^{-x} &gt; 0\)</span>, <span class="math notranslate nohighlight">\(\sigma(x)\)</span> is always between 0
and 1.</p>
<p>The <span class="math notranslate nohighlight">\(\sigma\)</span> function has a special property: the derivative of
<span class="math notranslate nohighlight">\(\sigma(x)\)</span> can be calculated easily:</p>
<div class="math notranslate nohighlight">
\begin{eqnarray}
&amp; \frac{d \sigma(x)}{d x} \\
&amp; =  \frac{d}{dx} (\frac{1}{1 + e^{-x}}) \\
&amp; = - \frac{1}{(1 + e^{-x})^2} \frac{d}{dx} (1 + e^{-x}) \\
&amp; =  \frac{e^{-x}}{(1 + e^{-x})^2} \\
&amp; = \frac{1}{1 + e^{-x}} \frac{e^{-x}}{1 + e^{-x}} \\
&amp; = \frac{1}{1 + e^{-x}} \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
&amp; = \sigma(x) (1 - \sigma(x)).
\end{eqnarray}</div><p>What does this mean? It means that we can calculate <span class="math notranslate nohighlight">\(\frac{d
\sigma(x)}{d x}\)</span> if we know <span class="math notranslate nohighlight">\(\sigma(x)\)</span> <em>without</em> knowing the
value of <span class="math notranslate nohighlight">\(x\)</span>.  If the value of <span class="math notranslate nohighlight">\(\sigma(x)\)</span> is <span class="math notranslate nohighlight">\(v\)</span>
for a particular value of <span class="math notranslate nohighlight">\(x\)</span>, then the derivative of
is <span class="math notranslate nohighlight">\(v (1-v)\)</span>.</p>
<p>Consider the following neuron. It has two inputs with values 0.99 and
0.01 respectively.  The weights are 0.8 and 0.2. The bias is 0.3.</p>
<div class="figure align-default" id="id6">
<img alt="../_images/neuralnet1.png" src="../_images/neuralnet1.png" />
<p class="caption"><span class="caption-number">Figure 69 </span><span class="caption-text">Example to calculate a neuron’s output</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>The value of <span class="math notranslate nohighlight">\(s\)</span> is 0.3 + 0.99 <span class="math notranslate nohighlight">\(\times\)</span> 0.8 + 0.01 <span class="math notranslate nohighlight">\(\times\)</span> 0.2 = 1.094.
After the <em>sigmoid</em> function, this neuron’s output is</p>
<p><span class="math notranslate nohighlight">\(\frac{1}{1+ e^{-1.094}} = 0.7491342\)</span>.</p>
<p>Next, consider a complete neural network with all weights and biases:</p>
<div class="figure align-default" id="id7">
<img alt="../_images/neuralnet2.png" src="../_images/neuralnet2.png" />
<p class="caption"><span class="caption-number">Figure 70 </span><span class="caption-text">Example to calculate a neural network’s outputs</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>The following table shows how to calculate the outputs of the
four neurons in the hidden layer:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 11%" />
<col style="width: 12%" />
<col style="width: 35%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>neuron</p></th>
<th class="head"><p>input</p></th>
<th class="head"><p>weight</p></th>
<th class="head"><p>bias</p></th>
<th class="head"><p>sum</p></th>
<th class="head"><p>output</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="2"><p>0</p></td>
<td><p>0.99</p></td>
<td><p>0.7</p></td>
<td rowspan="2"><p>0.1</p></td>
<td rowspan="2"><p>0.798</p></td>
<td rowspan="2"><p>0.689546499265968</p></td>
</tr>
<tr class="row-odd"><td><p>0.01</p></td>
<td><p>0.5</p></td>
</tr>
<tr class="row-even"><td rowspan="2"><p>1</p></td>
<td><p>0.99</p></td>
<td><p>0.8</p></td>
<td rowspan="2"><p>0.3</p></td>
<td rowspan="2"><p>1.094</p></td>
<td rowspan="2"><p>0.749134199078648</p></td>
</tr>
<tr class="row-odd"><td><p>0.01</p></td>
<td><p>0.2</p></td>
</tr>
<tr class="row-even"><td rowspan="2"><p>2</p></td>
<td><p>0.99</p></td>
<td><p>0.8</p></td>
<td rowspan="2"><p>0.5</p></td>
<td rowspan="2"><p>1.2955</p></td>
<td rowspan="2"><p>0.785076666300568</p></td>
</tr>
<tr class="row-odd"><td><p>0.01</p></td>
<td><p>0.35</p></td>
</tr>
<tr class="row-even"><td rowspan="2"><p>3</p></td>
<td><p>0.99</p></td>
<td><p>0.6</p></td>
<td rowspan="2"><p>0.4</p></td>
<td rowspan="2"><p>0.9965</p></td>
<td rowspan="2"><p>0.730369880613158</p></td>
</tr>
<tr class="row-odd"><td><p>0.01</p></td>
<td><p>0.25</p></td>
</tr>
</tbody>
</table>
<p>How are these numbers calculated. This is the procedure for
the first neuron:</p>
<p><span class="math notranslate nohighlight">\(s = 0.1 + 0.99 \times 0.7 + 0.01 \times 0.5 = 0.798\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{1}{1 + e^{-0.798}} = 0.689546499265968\)</span></p>
<p>The next table shows how the outputs are calculated.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 25%" />
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 22%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>neuron</p></th>
<th class="head"><p>input</p></th>
<th class="head"><p>weight</p></th>
<th class="head"><p>bias</p></th>
<th class="head"><p>sum</p></th>
<th class="head"><p>output</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="4"><p>0</p></td>
<td><p>0.689546499265968</p></td>
<td><p>0.33</p></td>
<td rowspan="4"><p>0.27</p></td>
<td rowspan="4"><p>1.5473332709399</p></td>
<td rowspan="4"><p>0.824528239528756</p></td>
</tr>
<tr class="row-odd"><td><p>0.7491341990786481</p></td>
<td><p>0.16</p></td>
</tr>
<tr class="row-even"><td><p>0.785076666300568</p></td>
<td><p>0.31</p></td>
</tr>
<tr class="row-odd"><td><p>0.730369880613158</p></td>
<td><p>0.94</p></td>
</tr>
<tr class="row-even"><td rowspan="4"><p>1</p></td>
<td><p>0.689546499265968</p></td>
<td><p>0.28</p></td>
<td rowspan="4"><p>0.45</p></td>
<td rowspan="4"><p>1.75723688345954</p></td>
<td rowspan="4"><p>0.852863261520065</p></td>
</tr>
<tr class="row-odd"><td><p>0.7491341990786481</p></td>
<td><p>0.76</p></td>
</tr>
<tr class="row-even"><td><p>0.785076666300568</p></td>
<td><p>0.48</p></td>
</tr>
<tr class="row-odd"><td><p>0.730369880613158</p></td>
<td><p>0.23</p></td>
</tr>
</tbody>
</table>
<p>The outputs of the two neurons are 0.824528239528756 and 0.852863261520065.</p>
</div>
<div class="section" id="gradient-descent-in-neural-networks-backpropagation">
<h3>Gradient Descent in Neural Networks (Backpropagation)<a class="headerlink" href="#gradient-descent-in-neural-networks-backpropagation" title="Permalink to this headline">¶</a></h3>
<p>How can the concept of gradient descent be used to make neural
networks produce the desired outputs? The answer is to adjust the
weights and the biases. More precisely, the adjustment starts from the
outputs toward the inputs and it is called <em>back propagation</em>.  For
the three layer networks shown above, the changes start from
<span class="math notranslate nohighlight">\(weight[1]\)</span> based on the differences of the desired and the
actual outputs. After adjusting <span class="math notranslate nohighlight">\(weight[1]\)</span>, the method adjusts
<span class="math notranslate nohighlight">\(weight[0]\)</span> between the hidden layer and the input layer.</p>
<p>When the actual outputs are different from the expected outputs, the
difference is called the <em>error</em>, usually expressed as <span class="math notranslate nohighlight">\(E\)</span>.
Consider the following definition of error:</p>
<p><span class="math notranslate nohighlight">\(E = \underset{k=0}{\overset{t-1}{\sum}} \frac{1}{2} (exv_k - aco_k)^2\)</span>,</p>
<p>Here <span class="math notranslate nohighlight">\(exv_k\)</span> is the expect value and <span class="math notranslate nohighlight">\(aco_k\)</span> is the actual
output of the <span class="math notranslate nohighlight">\(k^{th}\)</span> neuron (among <span class="math notranslate nohighlight">\(t\)</span> neurons).  This
definition adds the squares of errors from all output neurons. Do not
worry about the constant <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> because it is for
convenience here.  It will be cancelled later.</p>
<p><em>Gradient descent</em> can also be used to reduce the errors.  The process
is more complex than the previous examples because neural networks are
more complex: We can change the weights but their effects go through
the non-linear activiation function.  It is not easy expressing the
relationships between the error and the weights.  In other words, the
effects of weights go through a composition of functions (additions
first and then the non-linear function).</p>
<div class="section" id="chain-rule">
<h4>Chain Rule<a class="headerlink" href="#chain-rule" title="Permalink to this headline">¶</a></h4>
<p>Before explaining how to adjust the weights, let’s review function
composition.  Consider that <span class="math notranslate nohighlight">\(f(x)\)</span> and <span class="math notranslate nohighlight">\(g(x)\)</span> are
functions.  We can create a <em>composition of these two functions</em>. For
example, suppose <span class="math notranslate nohighlight">\(f(x) = 3 x + 8\)</span> and <span class="math notranslate nohighlight">\(g(x) = (x + 1) ^
2\)</span>. The composition of <span class="math notranslate nohighlight">\(f(x)\)</span> and <span class="math notranslate nohighlight">\(g(x)\)</span> can be written
as:</p>
<p><span class="math notranslate nohighlight">\((f \circ g)(x) = f(g(x)) = f((x+1)^2) = 3 (x + 1) ^ 2 + 8 = 3 x^2 + 6 x + 11\)</span>.</p>
<p>In this case, <span class="math notranslate nohighlight">\(g(x)\)</span> is applied first and then <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
<p>It is also possible to switch the order of the composition, written differently:</p>
<p><span class="math notranslate nohighlight">\((g \circ f)(x) = g(f(x)) = g(3x + 8) = ((3x + 8) + 1) ^ 2 = 9 x^2 + 54 x + 81\)</span>.</p>
<p>In this case, <span class="math notranslate nohighlight">\(f(x)\)</span> is applied first and then <span class="math notranslate nohighlight">\(g(x)\)</span>.</p>
<p>What happens if we want to calculate the effect of changes in
<span class="math notranslate nohighlight">\(x\)</span> to the composite function <span class="math notranslate nohighlight">\((f \circ g)(x)\)</span>? We need to
apply the <em>chain rule</em>.</p>
<p><span class="math notranslate nohighlight">\((f \circ g)'(x) = f'(g(x)) \cdot g'(x)\)</span>.</p>
<p>Let’s validate this by using the example:</p>
<p><span class="math notranslate nohighlight">\((f \circ g)(x) = 3 x ^ 2 + 6 x + 11\)</span>.</p>
<p>Thus, <span class="math notranslate nohighlight">\((f \circ g)'(x) = 6 x + 6\)</span>.</p>
<p>Next, we calculate <span class="math notranslate nohighlight">\(f'(x) = 3\)</span> and <span class="math notranslate nohighlight">\(g'(x) = 2 x + 2\)</span>.</p>
<p><span class="math notranslate nohighlight">\((f \circ g)'(x) = f'(g(x)) \cdot g'(x) = 3 (2 x + 2) = 6x + 6\)</span>.</p>
</div>
<div class="section" id="error-and-weights-between-output-and-hidden-layers">
<h4>Error and Weights between Output and Hidden Layers<a class="headerlink" href="#error-and-weights-between-output-and-hidden-layers" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default" id="id8">
<img alt="../_images/threelayers.png" src="../_images/threelayers.png" />
<p class="caption"><span class="caption-number">Figure 71 </span><span class="caption-text">Definitions of the symbols</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<p>Symbols and their meanings:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 23%" />
<col style="width: 77%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>symbol</p></th>
<th class="head"><p>meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(r\)</span></p></td>
<td><p>number of neurons in the input layer</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(s\)</span></p></td>
<td><p>number of neurons in the hidden layer</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(t\)</span></p></td>
<td><p>number of neurons in the output layer</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(i\)</span></p></td>
<td><p>index of a neuron in the input layer <span class="math notranslate nohighlight">\(0 \le i &lt; r\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(j\)</span></p></td>
<td><p>index of a neuron in the hidden layer <span class="math notranslate nohighlight">\(0 \le j &lt; s\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(k\)</span></p></td>
<td><p>index of a neuron in the output layer <span class="math notranslate nohighlight">\(0 \le k &lt; t\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_j\)</span></p></td>
<td><p>bias of the <span class="math notranslate nohighlight">\(j^{th}\)</span> neuron in the hidden layer</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(b_k\)</span></p></td>
<td><p>bias of the <span class="math notranslate nohighlight">\(k^{th}\)</span> neuron in the output layer</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(u_i\)</span></p></td>
<td><p>input to the <span class="math notranslate nohighlight">\(i^{th}\)</span> neuron in the input layer</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(x_j\)</span></p></td>
<td><p>input to the <span class="math notranslate nohighlight">\(j^{th}\)</span> neuron in the hidden layer</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(y_j\)</span></p></td>
<td><p>output of the <span class="math notranslate nohighlight">\(j^{th}\)</span> neuron in the hidden layer</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(z_k\)</span></p></td>
<td><p>input to the <span class="math notranslate nohighlight">\(k^{th}\)</span> neuron in the output layer</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(aco_k\)</span></p></td>
<td><p>output of the <span class="math notranslate nohighlight">\(k^{th}\)</span> neuron in the output layer</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(exv_k\)</span></p></td>
<td><p>expected value of the <span class="math notranslate nohighlight">\(k^{th}\)</span> neuron in the output layer</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\psi_{i,j}\)</span></p></td>
<td><p>weight connecting the <span class="math notranslate nohighlight">\(i^{th}\)</span> and <span class="math notranslate nohighlight">\(j^{th}\)</span> neurons</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(w_{j, k}\)</span></p></td>
<td><p>weight connecting the <span class="math notranslate nohighlight">\(j^{th}\)</span> and <span class="math notranslate nohighlight">\(k^{th}\)</span> neurons</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\eta\)</span></p></td>
<td><p>learning rate</p></td>
</tr>
</tbody>
</table>
<p>Here are the equations expressing the relationships among these terms:</p>
<p>Error:
<span class="math notranslate nohighlight">\(E = \underset{k=0}{\overset{t-1}{\sum}} \frac{1}{2} (aco_k - exv_k)^2\)</span>,</p>
<p>Actual output of the <span class="math notranslate nohighlight">\(k^{th}\)</span> neuron:
<span class="math notranslate nohighlight">\(aco_k = \sigma(z_k)\)</span></p>
<p>Input to the <span class="math notranslate nohighlight">\(k^{th}\)</span> neuron:
<span class="math notranslate nohighlight">\(z_k = b_k + \underset{j=0}{\overset{s-1}{\sum}} y_j w_{j,k}\)</span></p>
<p>Output of the <span class="math notranslate nohighlight">\(j^{th}\)</span> neuron:
<span class="math notranslate nohighlight">\(y_j = \sigma(x_j)\)</span></p>
<p>Input to the <span class="math notranslate nohighlight">\(j^{th}\)</span> neuron:
<span class="math notranslate nohighlight">\(x_j = \beta_j + \underset{i=0}{\overset{r-1}{\sum}} u_i \psi_{i,j}\)</span></p>
<p>Now, we can apply the chain rule to calculate the relationship between
the error and a weight or a bias.  Consider the output of a particular
neuron.  The expected value is a constant and its derivative is
zero. Thus, we can ignore <span class="math notranslate nohighlight">\(\frac{\partial exv_k}{\partial
w_{j,k}}\)</span>. Instead, we need to worry about only the relationship
between the actual output <span class="math notranslate nohighlight">\(aco_k\)</span> and a weight <span class="math notranslate nohighlight">\(w_{j,k}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial E}{\partial w_{j,k}} = \frac{\partial E}{\partial aco_k} \frac{\partial aco_k}{\partial w_{j,k}}\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial E}{\partial aco_k} = (aco_k - exv_k)\)</span>, because
<span class="math notranslate nohighlight">\(aco_k\)</span> does not depend on any other neuron in the output layer.</p>
<p>As you can see, the constant <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> has been cancelled.</p>
<p>Applying the chain rule again:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial aco_k}{\partial w_{j,k}} = \frac{\partial aco_k}{\partial z_k} \frac{\partial z_k}{\partial w_{j,k}}\)</span>.</p>
<p>As explained earlier,
<span class="math notranslate nohighlight">\(\frac{\partial aco_k}{\partial z_k} = \sigma(z_k)(1 - \sigma(z_k)) = aco _k(1 - aco_k)\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial z_k}{\partial w_{j,k}} = y_j\)</span>.</p>
<p>Now, we can put everything together:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial E}{\partial w_{j,k}} = (aco_k - exv_k) aco_k (1 - aco_k) y_j\)</span>.</p>
<p>Using gradient descent, we want to change the weight</p>
<p><span class="math notranslate nohighlight">\(\Delta w_{j,k} = - \eta \frac{\partial E}{\partial w_{j,k}} = - \eta (aco_k - exv_k) aco_k (1 - aco_k) y_j\)</span>.</p>
<p>Similarly,</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial E}{\partial b_k} = (aco_k - exv_k) aco_k (1 - aco_k)\)</span>.</p>
<p>To change the bias:</p>
<p><span class="math notranslate nohighlight">\(\Delta b_k = - \eta \frac{\partial E}{\partial b_k} = - \eta (aco_k - exv_k) aco_k (1 - aco_k)\)</span>.</p>
</div>
<div class="section" id="error-and-weights-between-hidden-and-input-layers">
<h4>Error and Weights between Hidden and Input Layers<a class="headerlink" href="#error-and-weights-between-hidden-and-input-layers" title="Permalink to this headline">¶</a></h4>
<p>Next, we adjust the weights between the input and the hidden layer.
The problem of the hidden layer is that we do not have expected values
for the neurons. Instead, we must rely on the expected values at the
output layer.  This is called <em>back propagation</em>: propagating the
expected values from the output layer to the hidden layer in order to
adjust the weights.  Consider the weight <span class="math notranslate nohighlight">\(\psi_{i,j}\)</span> between
the <span class="math notranslate nohighlight">\(i^{th}\)</span> neuron of the input layer and the <span class="math notranslate nohighlight">\(j^{th}\)</span>
neuron in the hidden layer.  This weight may affect every neuron in
the output layer.</p>
<p>Continue from the previous derivation, the output of the neuron
<span class="math notranslate nohighlight">\(y_j\)</span> depends on the values of the input layer, the weights
between the input layer and the hidden layer, and the bias.  To avoid
confusion, here <span class="math notranslate nohighlight">\(\psi\)</span>, instead of <span class="math notranslate nohighlight">\(w\)</span>, is used to
express the weight between the input and the hidden layer.</p>
<p>How does the weight <span class="math notranslate nohighlight">\(\psi_{i,j}\)</span> affects the error? We can apply
the chain rule again:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial E}{\partial \psi_{i,j}} = \frac{\partial
}{\partial \psi_{i,j}} \underset{k=0}{\overset{t-1}{\sum}} \frac{1}{2}
(aco_k - exv_k)^2 = \underset{k=0}{\overset{t-1}{\sum}} (aco_k -
exv_k) \frac{\partial aco_k}{\partial \psi_{i,j}}\)</span></p>
<p>Our next step is to calculate <span class="math notranslate nohighlight">\(\frac{\partial aco_k}{\partial \psi_{i,j}}\)</span></p>
<p>We can apply the chair rule again:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial aco_k}{\partial \psi_{i,j}} = \frac{\partial
aco_k}{\partial z_k} \frac{\partial z_k}{\partial \psi_{i,j}} = aco_k
(1 - aco_k) \frac{\partial z_k}{\partial \psi_{i,j}}\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial z_k}{\partial \psi_{i,j}} = \frac{\partial      z_k}{\partial y_j} \frac{\partial y_j}{\partial \psi_{i,j}} =      w_{j, k} \frac{\partial y_j}{\partial \psi_{i,j}}\)</span></p>
<p>Apply the chain rule and we can get</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial y_j}{\partial \psi_{i,j}} = \frac{\partial y_j}{\partial x_j} \frac{\partial x_j}{\partial \psi_{i,j}} = y_j (1 - y_j) \frac{\partial x_j}{\partial \psi_{i,j}}\)</span></p>
<p>Finally,
<span class="math notranslate nohighlight">\(\frac{\partial x_j}{\partial \psi_{i,j}} = u_i\)</span>.</p>
<p>Put everything together:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial E}{\partial \psi_{i,j}} = \underset{k=0}{\overset{t-1}{\sum}} (aco_k - exv_k)  aco_k (1 - aco_k)  w_{j, k} y_j (1 - y_j) u_i\)</span></p>
<p>Using gradient descent, we want to change the weight</p>
<p><span class="math notranslate nohighlight">\(\Delta \psi_{i,j} = -\eta \underset{k=0}{\overset{t-1}{\sum}} (aco_k - exv_k)  aco_k (1 - aco_k)  w_{j, k} y_j (1 - y_j) u_i\)</span></p>
<p>Similarly,</p>
<p><span class="math notranslate nohighlight">\(\Delta \beta_j = -\eta \underset{k=0}{\overset{t-1}{\sum}} (aco_k - exv_k)  aco_k (1 - aco_k)  w_{j, k} y_j (1 - y_j)\)</span>.</p>
</div>
</div>
<div class="section" id="neural-networks-as-logic-gates">
<h3>Neural Networks as Logic Gates<a class="headerlink" href="#neural-networks-as-logic-gates" title="Permalink to this headline">¶</a></h3>
<p>After understanding the operations of neural networks, it is time to use neural networks
doing something useful. This example considers building a two-input logic gate.
Each gate receives either false (also represented by 0) or true (also represented by 1).
Commonly used gates are shown in the table below:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 28%" />
<col style="width: 28%" />
<col style="width: 16%" />
<col style="width: 13%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>input 0</p></th>
<th class="head"><p>input 1</p></th>
<th class="head"><p>AND</p></th>
<th class="head"><p>OR</p></th>
<th class="head"><p>XOR</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Prior studies have shown that it would not be possible make an XOR gate without
any hidden layer. We won’t talk about that here. Instead, right now we focus on how to use graident descent to create logic gates.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="chapter_svm.html" class="btn btn-neutral float-right" title="Support Vector Machines" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="chapter_supervised.html" class="btn btn-neutral float-left" title="Supervised Machine Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Yung-Hsiang Lu and George K. Thiruvathukal

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>